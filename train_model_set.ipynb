{"cells":[{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":99,"status":"ok","timestamp":1768676063373,"user":{"displayName":"CHAITRA P","userId":"09561147720709225417"},"user_tz":-330},"id":"a046c06a","outputId":"a5ae246d-cd59-4802-ffff-8e300cd38870"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ All libraries imported successfully!\n","‚úÖ Folders created: model/, data/\n","\n","======================================================================\n","üì• LOADING HEART DISEASE DATASET\n","======================================================================\n","‚úÖ Dataset loaded successfully from: heart.csv\n","\n","======================================================================\n","üìã DATASET VERIFICATION - ASSIGNMENT REQUIREMENTS\n","======================================================================\n","Dataset Shape: (1025, 14)\n","Rows (Instances): 1025\n","Columns (Features + Target): 14\n","Features (excluding target): 13\n","\n","üìä REQUIREMENT CHECK:\n","   Minimum Features Required: 12\n","   Your Dataset Features: 13\n","   Status: ‚úÖ PASS\n","\n","   Minimum Instances Required: 500\n","   Your Dataset Instances: 1025\n","   Status: ‚úÖ PASS\n","\n","üéâ DATASET MEETS ALL ASSIGNMENT REQUIREMENTS!\n","======================================================================\n","\n","üìä FIRST 5 ROWS:\n","   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n","0   52    1   0       125   212    0        1      168      0      1.0      2   \n","1   53    1   0       140   203    1        0      155      1      3.1      0   \n","2   70    1   0       145   174    0        1      125      1      2.6      0   \n","3   61    1   0       148   203    0        1      161      0      0.0      2   \n","4   62    0   0       138   294    1        1      106      0      1.9      1   \n","\n","   ca  thal  target  \n","0   2     3       0  \n","1   0     3       0  \n","2   0     3       0  \n","3   1     3       0  \n","4   3     2       0  \n","\n","üìã COLUMN NAMES:\n","['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n","\n","üìà DATA TYPES:\n","age           int64\n","sex           int64\n","cp            int64\n","trestbps      int64\n","chol          int64\n","fbs           int64\n","restecg       int64\n","thalach       int64\n","exang         int64\n","oldpeak     float64\n","slope         int64\n","ca            int64\n","thal          int64\n","target        int64\n","dtype: object\n","\n","‚ùì MISSING VALUES:\n","‚úÖ No missing values found - Dataset is clean!\n","\n","üéØ TARGET VARIABLE DISTRIBUTION:\n","target\n","0    499\n","1    526\n","Name: count, dtype: int64\n","\n","Percentage:\n","target\n","1    51.32\n","0    48.68\n","Name: proportion, dtype: float64\n","\n","‚úÖ SECTION 2 COMPLETE - DATASET LOADED & VERIFIED!\n","======================================================================\n","\n","======================================================================\n","‚öôÔ∏è DATA PREPROCESSING\n","======================================================================\n","\n","1Ô∏è‚É£ FEATURE-TARGET SEPARATION:\n","----------------------------------------------------------------------\n","Features (X): (1025, 13)\n","Target (y): (1025,)\n","\n","2Ô∏è‚É£ TRAIN-TEST SPLIT (BEFORE SCALING):\n","----------------------------------------------------------------------\n","‚ö†Ô∏è CRITICAL: Splitting BEFORE scaling to prevent data leakage!\n","\n","‚úÖ Split completed:\n","   Training set: 820 samples (80.0%)\n","   Test set:     205 samples (20.0%)\n","\n","üìä Class distribution check:\n","   Original:  {1: 526, 0: 499}\n","   Training:  {1: 421, 0: 399}\n","   Test:      {1: 105, 0: 100}\n","\n","3Ô∏è‚É£ FEATURE SCALING (AFTER SPLIT):\n","----------------------------------------------------------------------\n","‚úÖ Scaling AFTER split - No data leakage!\n","\n","   a) Fitting scaler on TRAINING data only...\n","      ‚úÖ Scaler learned mean & std from training data\n","\n","   b) Transforming TEST data using training statistics...\n","      ‚úÖ Test data transformed (no refitting)\n","\n","üìä Scaling Verification:\n","   Training data mean: -0.000000 (should be ~0)\n","   Training data std:  1.000610 (should be ~1)\n","\n","‚úÖ Feature scaling completed successfully!\n","\n","4Ô∏è‚É£ SAVING FOR STREAMLIT APP:\n","----------------------------------------------------------------------\n","‚úÖ Scaler saved: model/scaler.pkl\n","‚úÖ Feature names saved: model/feature_names.pkl\n","‚úÖ Test sample saved: data/test_data.csv\n","\n","5Ô∏è‚É£ INITIALIZING RESULTS STORAGE:\n","----------------------------------------------------------------------\n","‚úÖ Results storage initialized\n","\n","======================================================================\n","‚úÖ PREPROCESSING COMPLETE - NO DATA LEAKAGE!\n","======================================================================\n","\n","üöÄ Ready for model training!\n","\n","======================================================================\n","ü§ñ TRAINING 6 CLASSIFICATION MODELS\n","======================================================================\n"]}],"source":["# ============================================================================\n","# MACHINE LEARNING ASSIGNMENT 2 - HEART DISEASE CLASSIFICATION\n","# ============================================================================\n","# Student: [Your Name]\n","# Course: M.Tech (AIML/DSE) - Machine Learning\n","# Dataset: Heart Disease UCI\n","# Models: 6 Classification Models\n","# ============================================================================\n","\n","# ============================================================================\n","# SECTION 1: IMPORT LIBRARIES\n","# ============================================================================\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","import joblib\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Scikit-learn imports\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# XGBoost\n","from xgboost import XGBClassifier\n","\n","# Metrics\n","from sklearn.metrics import (\n","    accuracy_score,\n","    roc_auc_score,\n","    precision_score,\n","    recall_score,\n","    f1_score,\n","    matthews_corrcoef,\n","    confusion_matrix,\n","    classification_report\n",")\n","\n","print(\"‚úÖ All libraries imported successfully!\")\n","\n","# ============================================================================\n","# CREATE REQUIRED FOLDERS\n","# ============================================================================\n","\n","os.makedirs('model', exist_ok=True)\n","os.makedirs('data', exist_ok=True)\n","\n","print(\"‚úÖ Folders created: model/, data/\")\n","\n","# ============================================================================\n","# SECTION 2: LOAD HEART DISEASE DATASET\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"üì• LOADING HEART DISEASE DATASET\")\n","print(\"=\"*70)\n","\n","# Load dataset (assumes heart.csv is in working directory)\n","try:\n","    df = pd.read_csv(\"heart.csv\")\n","    print(\"‚úÖ Dataset loaded successfully from: heart.csv\")\n","except FileNotFoundError:\n","    print(\"‚ùå heart.csv not found!\")\n","    print(\"Please download from: https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset\")\n","    raise\n","\n","# ============================================================================\n","# DATASET VERIFICATION\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"üìã DATASET VERIFICATION - ASSIGNMENT REQUIREMENTS\")\n","print(\"=\"*70)\n","\n","print(f\"Dataset Shape: {df.shape}\")\n","print(f\"Rows (Instances): {df.shape[0]}\")\n","print(f\"Columns (Features + Target): {df.shape[1]}\")\n","print(f\"Features (excluding target): {df.shape[1] - 1}\")\n","\n","# Check requirements\n","features_count = df.shape[1] - 1\n","instances_count = df.shape[0]\n","\n","print(f\"\\nüìä REQUIREMENT CHECK:\")\n","print(f\"   Minimum Features Required: 12\")\n","print(f\"   Your Dataset Features: {features_count}\")\n","print(f\"   Status: {'‚úÖ PASS' if features_count >= 12 else '‚ùå FAIL'}\")\n","\n","print(f\"\\n   Minimum Instances Required: 500\")\n","print(f\"   Your Dataset Instances: {instances_count}\")\n","print(f\"   Status: {'‚úÖ PASS' if instances_count >= 500 else '‚ùå FAIL'}\")\n","\n","if features_count >= 12 and instances_count >= 500:\n","    print(\"\\nüéâ DATASET MEETS ALL ASSIGNMENT REQUIREMENTS!\")\n","else:\n","    print(\"\\n‚ö†Ô∏è WARNING: Dataset does not meet requirements!\")\n","\n","print(\"=\"*70)\n","\n","# ============================================================================\n","# DATASET OVERVIEW\n","# ============================================================================\n","\n","print(\"\\nüìä FIRST 5 ROWS:\")\n","print(df.head())\n","\n","print(\"\\nüìã COLUMN NAMES:\")\n","print(list(df.columns))\n","\n","print(\"\\nüìà DATA TYPES:\")\n","print(df.dtypes)\n","\n","print(\"\\n‚ùì MISSING VALUES:\")\n","missing = df.isnull().sum()\n","total_missing = missing.sum()\n","\n","if total_missing == 0:\n","    print(\"‚úÖ No missing values found - Dataset is clean!\")\n","else:\n","    print(f\"‚ö†Ô∏è Found {total_missing} missing values:\")\n","    print(missing[missing > 0])\n","\n","print(\"\\nüéØ TARGET VARIABLE DISTRIBUTION:\")\n","print(df['target'].value_counts().sort_index())\n","print(\"\\nPercentage:\")\n","print(df['target'].value_counts(normalize=True).round(4) * 100)\n","\n","print(\"\\n‚úÖ SECTION 2 COMPLETE - DATASET LOADED & VERIFIED!\")\n","print(\"=\"*70)\n","\n","# ============================================================================\n","# SECTION 3: DATA PREPROCESSING & TRAIN-TEST SPLIT\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"‚öôÔ∏è DATA PREPROCESSING\")\n","print(\"=\"*70)\n","\n","# ============================================================================\n","# STEP 1: SEPARATE FEATURES AND TARGET\n","# ============================================================================\n","\n","print(\"\\n1Ô∏è‚É£ FEATURE-TARGET SEPARATION:\")\n","print(\"-\"*70)\n","\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","print(f\"Features (X): {X.shape}\")\n","print(f\"Target (y): {y.shape}\")\n","\n","# ============================================================================\n","# STEP 2: TRAIN-TEST SPLIT (BEFORE SCALING!)\n","# ============================================================================\n","\n","print(\"\\n2Ô∏è‚É£ TRAIN-TEST SPLIT (BEFORE SCALING):\")\n","print(\"-\"*70)\n","print(\"‚ö†Ô∏è CRITICAL: Splitting BEFORE scaling to prevent data leakage!\")\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y,\n","    test_size=0.2,        # 20% for testing\n","    random_state=42,      # Reproducibility\n","    stratify=y            # Maintain class balance\n",")\n","\n","print(f\"\\n‚úÖ Split completed:\")\n","print(f\"   Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(df)*100:.1f}%)\")\n","print(f\"   Test set:     {X_test.shape[0]} samples ({X_test.shape[0]/len(df)*100:.1f}%)\")\n","\n","# Verify class balance maintained\n","print(f\"\\nüìä Class distribution check:\")\n","print(f\"   Original:  {y.value_counts().to_dict()}\")\n","print(f\"   Training:  {y_train.value_counts().to_dict()}\")\n","print(f\"   Test:      {y_test.value_counts().to_dict()}\")\n","\n","# ============================================================================\n","# STEP 3: FEATURE SCALING (AFTER SPLIT!)\n","# ============================================================================\n","\n","print(\"\\n3Ô∏è‚É£ FEATURE SCALING (AFTER SPLIT):\")\n","print(\"-\"*70)\n","print(\"‚úÖ Scaling AFTER split - No data leakage!\")\n","\n","# Initialize scaler\n","scaler = StandardScaler()\n","\n","# Fit on training data ONLY, then transform\n","print(\"\\n   a) Fitting scaler on TRAINING data only...\")\n","X_train_scaled = scaler.fit_transform(X_train)\n","print(\"      ‚úÖ Scaler learned mean & std from training data\")\n","\n","# Transform test data using training statistics\n","print(\"\\n   b) Transforming TEST data using training statistics...\")\n","X_test_scaled = scaler.transform(X_test)\n","print(\"      ‚úÖ Test data transformed (no refitting)\")\n","\n","# Convert back to DataFrame\n","X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n","X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n","\n","# Verify scaling worked\n","print(f\"\\nüìä Scaling Verification:\")\n","print(f\"   Training data mean: {X_train_scaled.mean().mean():.6f} (should be ~0)\")\n","print(f\"   Training data std:  {X_train_scaled.std().mean():.6f} (should be ~1)\")\n","\n","print(\"\\n‚úÖ Feature scaling completed successfully!\")\n","\n","# ============================================================================\n","# STEP 4: SAVE PREPROCESSING OBJECTS FOR STREAMLIT\n","# ============================================================================\n","\n","print(\"\\n4Ô∏è‚É£ SAVING FOR STREAMLIT APP:\")\n","print(\"-\"*70)\n","\n","# Save scaler (CRITICAL for Streamlit!)\n","joblib.dump(scaler, 'model/scaler.pkl')\n","print(\"‚úÖ Scaler saved: model/scaler.pkl\")\n","\n","# Save feature names\n","feature_names = list(X.columns)\n","joblib.dump(feature_names, 'model/feature_names.pkl')\n","print(\"‚úÖ Feature names saved: model/feature_names.pkl\")\n","\n","# Save test data sample for demo\n","test_sample = X_test_scaled.head(100).copy()\n","test_sample['target'] = y_test.head(100).values\n","test_sample.to_csv('data/test_data.csv', index=False)\n","print(\"‚úÖ Test sample saved: data/test_data.csv\")\n","\n","# ============================================================================\n","# STEP 5: INITIALIZE RESULTS STORAGE\n","# ============================================================================\n","\n","print(\"\\n5Ô∏è‚É£ INITIALIZING RESULTS STORAGE:\")\n","print(\"-\"*70)\n","\n","results = {\n","    'Model': [],\n","    'Accuracy': [],\n","    'AUC': [],\n","    'Precision': [],\n","    'Recall': [],\n","    'F1': [],\n","    'MCC': []\n","}\n","\n","trained_models = {}\n","model_predictions = {}\n","model_confusion_matrices = {}\n","model_classification_reports = {}\n","\n","print(\"‚úÖ Results storage initialized\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"‚úÖ PREPROCESSING COMPLETE - NO DATA LEAKAGE!\")\n","print(\"=\"*70)\n","print(\"\\nüöÄ Ready for model training!\")\n","\n","# ============================================================================\n","# SECTION 4: TRAIN ALL 6 CLASSIFICATION MODELS\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"ü§ñ TRAINING 6 CLASSIFICATION MODELS\")\n","print(\"=\"*70)\n","\n","# ============================================================================\n","# DEFINE ALL MODELS (WITH PROPER REGULARIZATION TO AVOID OVERFITTING)\n","# ============================================================================\n","\n","models_to_train = {\n","    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n","    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=5, min_samples_split=10),\n","    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n","    'Naive Bayes': GaussianNB(),\n","    'Random Forest': RandomForestClassifier(\n","        n_estimators=100,\n","        random_state=42,\n","        max_depth=5,          # Reduced to prevent overfitting\n","        min_samples_split=10, # Added regularization\n","        min_samples_leaf=5    # Added regularization\n","    ),\n","    'XGBoost': XGBClassifier(\n","        n_estimators=100,\n","        random_state=42,\n","        max_depth=3,          # Reduced to prevent overfitting\n","        learning_rate=0.1,    # Added regularization\n","        eval_metric='logloss',\n","        use_label_encoder=False\n","    )\n","}\n","\n","# ============================================================================\n","# TRAINING FUNCTION\n","# ============================================================================\n","\n","def train_and_evaluate_model(model_name, model, X_train, X_test, y_train, y_test):\n","    \"\"\"Train a model and calculate all 6 evaluation metrics + confusion matrix + classification report.\"\"\"\n","\n","    print(f\"\\n{'='*70}\")\n","    print(f\"üîÑ Training: {model_name}\")\n","    print(f\"{'='*70}\")\n","\n","    # Train the model\n","    model.fit(X_train, y_train)\n","    print(f\"‚úÖ Model trained successfully\")\n","\n","    # Make predictions\n","    y_pred = model.predict(X_test)\n","    y_pred_proba = model.predict_proba(X_test)[:, 1]\n","\n","    # Calculate all 6 metrics\n","    metrics = {\n","        'Accuracy': accuracy_score(y_test, y_pred),\n","        'AUC': roc_auc_score(y_test, y_pred_proba),\n","        'Precision': precision_score(y_test, y_pred, zero_division=0),\n","        'Recall': recall_score(y_test, y_pred, zero_division=0),\n","        'F1': f1_score(y_test, y_pred, zero_division=0),\n","        'MCC': matthews_corrcoef(y_test, y_pred)\n","    }\n","\n","    # Display metrics\n","    print(f\"\\nüìä Evaluation Metrics:\")\n","    print(f\"   Accuracy:  {metrics['Accuracy']:.4f}\")\n","    print(f\"   AUC:       {metrics['AUC']:.4f}\")\n","    print(f\"   Precision: {metrics['Precision']:.4f}\")\n","    print(f\"   Recall:    {metrics['Recall']:.4f}\")\n","    print(f\"   F1 Score:  {metrics['F1']:.4f}\")\n","    print(f\"   MCC:       {metrics['MCC']:.4f}\")\n","\n","    # Confusion matrix\n","    cm = confusion_matrix(y_test, y_pred)\n","    print(f\"\\nüî¢ Confusion Matrix:\")\n","    print(f\"   TN: {cm[0,0]:3d}  |  FP: {cm[0,1]:3d}\")\n","    print(f\"   FN: {cm[1,0]:3d}  |  TP: {cm[1,1]:3d}\")\n","\n","    # Classification report\n","    report = classification_report(y_test, y_pred, target_names=['No Disease', 'Disease'])\n","    print(f\"\\nüìã Classification Report:\")\n","    print(report)\n","\n","    print(f\"\\n‚úÖ {model_name} completed!\")\n","\n","    return {\n","        'model': model,\n","        'metrics': metrics,\n","        'predictions': y_pred,\n","        'probabilities': y_pred_proba,\n","        'confusion_matrix': cm,\n","        'classification_report': report\n","    }\n"]},{"cell_type":"code","source":["# ============================================================================\n","# TRAIN ALL MODELS\n","# ============================================================================\n","\n","print(\"\\nüöÄ Starting training pipeline for all 6 models...\")\n","print(f\"Training set size: {X_train_scaled.shape[0]} samples\")\n","print(f\"Test set size: {X_test_scaled.shape[0]} samples\")\n","\n","model_results = {}\n","\n","for idx, (model_name, model) in enumerate(models_to_train.items(), 1):\n","    print(f\"\\n{'#'*70}\")\n","    print(f\"MODEL {idx}/6: {model_name.upper()}\")\n","    print(f\"{'#'*70}\")\n","\n","    # Train and evaluate\n","    result = train_and_evaluate_model(\n","        model_name=model_name,\n","        model=model,\n","        X_train=X_train_scaled,\n","        X_test=X_test_scaled,\n","        y_train=y_train,\n","        y_test=y_test\n","    )\n","\n","    # Store results\n","    model_results[model_name] = result\n","\n","    # Store in results dictionary for comparison table\n","    results['Model'].append(model_name)\n","    results['Accuracy'].append(round(result['metrics']['Accuracy'], 4))\n","    results['AUC'].append(round(result['metrics']['AUC'], 4))\n","    results['Precision'].append(round(result['metrics']['Precision'], 4))\n","    results['Recall'].append(round(result['metrics']['Recall'], 4))\n","    results['F1'].append(round(result['metrics']['F1'], 4))\n","    results['MCC'].append(round(result['metrics']['MCC'], 4))\n","\n","    # Store model, predictions, confusion matrix, and classification report\n","    trained_models[model_name] = result['model']\n","    model_predictions[model_name] = result['predictions']\n","    model_confusion_matrices[model_name] = result['confusion_matrix']\n","    model_classification_reports[model_name] = result['classification_report']\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"‚úÖ ALL 6 MODELS TRAINED SUCCESSFULLY!\")\n","print(\"=\"*70)\n","\n","# ============================================================================\n","# CREATE RESULTS COMPARISON TABLE (REQUIRED FOR README)\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"üìä MODEL COMPARISON TABLE (ALL 6 MODELS)\")\n","print(\"=\"*70)\n","\n","results_df = pd.DataFrame(results)\n","print(\"\\n\" + results_df.to_string(index=False))\n","\n","# Save comparison table\n","results_df.to_csv('model/model_comparison.csv', index=False)\n","print(\"\\n‚úÖ Comparison table saved: model/model_comparison.csv\")\n","\n","# ============================================================================\n","# IDENTIFY BEST PERFORMING MODELS\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"üèÜ BEST PERFORMING MODELS BY METRIC\")\n","print(\"=\"*70)\n","\n","for metric in ['Accuracy', 'AUC', 'Precision', 'Recall', 'F1', 'MCC']:\n","    best_idx = results_df[metric].idxmax()\n","    best_model = results_df.loc[best_idx, 'Model']\n","    best_score = results_df.loc[best_idx, metric]\n","    print(f\"{metric:12} : {best_model:25} ({best_score:.4f})\")\n","\n","# Find overall best model (by average)\n","print(\"\\n\" + \"=\"*70)\n","print(\"ü•á OVERALL BEST MODEL (by average performance)\")\n","print(\"=\"*70)\n","\n","results_df['Average'] = results_df[['Accuracy', 'AUC', 'Precision', 'Recall', 'F1', 'MCC']].mean(axis=1)\n","best_overall_idx = results_df['Average'].idxmax()\n","best_overall_model = results_df.loc[best_overall_idx, 'Model']\n","best_overall_score = results_df.loc[best_overall_idx, 'Average']\n","\n","print(f\"Best Model: {best_overall_model}\")\n","print(f\"Average Score: {best_overall_score:.4f}\")\n","\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"üíæ SAVING REQUIRED FILES FOR STREAMLIT APP\")\n","print(\"=\"*70)\n","\n","# 1. Save all 6 models (REQUIRED)\n","for model_name, model_obj in trained_models.items():\n","    filename = model_name.lower().replace(' ', '_').replace('-', '_') + '.pkl'\n","    filepath = os.path.join('model', filename)\n","    joblib.dump(model_obj, filepath)\n","    print(f\"‚úÖ Saved: {filepath}\")\n","\n","# 2. Save scaler (CRITICAL - needed to scale uploaded data in Streamlit)\n","joblib.dump(scaler, 'model/scaler.pkl')\n","print(\"‚úÖ Saved: model/scaler.pkl\")\n","\n","# 3. Save feature names (HELPFUL - for validation)\n","feature_names = list(X.columns)\n","joblib.dump(feature_names, 'model/feature_names.pkl')\n","print(\"‚úÖ Saved: model/feature_names.pkl\")\n","\n","# 4. Save comparison table (REQUIRED - for README.md)\n","results_df.to_csv('model/model_comparison.csv', index=False)\n","print(\"‚úÖ Saved: model/model_comparison.csv\")\n","\n","# 5. Save sample test data (OPTIONAL - for demo)\n","test_sample = X_test.head(100).copy()\n","test_sample['target'] = y_test.head(100).values\n","test_sample.to_csv('data/test_sample.csv', index=False)\n","print(\"‚úÖ Saved: data/test_sample.csv (for demo purposes)\")\n","\n","print(\"\\n‚úÖ All required files saved!\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"_PmUpiGVP4-t","executionInfo":{"status":"ok","timestamp":1768676063849,"user_tz":-330,"elapsed":475,"user":{"displayName":"CHAITRA P","userId":"09561147720709225417"}},"outputId":"717fe410-6a75-44f9-9289-0c8b77452319"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üöÄ Starting training pipeline for all 6 models...\n","Training set size: 820 samples\n","Test set size: 205 samples\n","\n","######################################################################\n","MODEL 1/6: LOGISTIC REGRESSION\n","######################################################################\n","\n","======================================================================\n","üîÑ Training: Logistic Regression\n","======================================================================\n","‚úÖ Model trained successfully\n","\n","üìä Evaluation Metrics:\n","   Accuracy:  0.8098\n","   AUC:       0.9298\n","   Precision: 0.7619\n","   Recall:    0.9143\n","   F1 Score:  0.8312\n","   MCC:       0.6309\n","\n","üî¢ Confusion Matrix:\n","   TN:  70  |  FP:  30\n","   FN:   9  |  TP:  96\n","\n","üìã Classification Report:\n","              precision    recall  f1-score   support\n","\n","  No Disease       0.89      0.70      0.78       100\n","     Disease       0.76      0.91      0.83       105\n","\n","    accuracy                           0.81       205\n","   macro avg       0.82      0.81      0.81       205\n","weighted avg       0.82      0.81      0.81       205\n","\n","\n","‚úÖ Logistic Regression completed!\n","\n","######################################################################\n","MODEL 2/6: DECISION TREE\n","######################################################################\n","\n","======================================================================\n","üîÑ Training: Decision Tree\n","======================================================================\n","‚úÖ Model trained successfully\n","\n","üìä Evaluation Metrics:\n","   Accuracy:  0.8732\n","   AUC:       0.9326\n","   Precision: 0.8624\n","   Recall:    0.8952\n","   F1 Score:  0.8785\n","   MCC:       0.7465\n","\n","üî¢ Confusion Matrix:\n","   TN:  85  |  FP:  15\n","   FN:  11  |  TP:  94\n","\n","üìã Classification Report:\n","              precision    recall  f1-score   support\n","\n","  No Disease       0.89      0.85      0.87       100\n","     Disease       0.86      0.90      0.88       105\n","\n","    accuracy                           0.87       205\n","   macro avg       0.87      0.87      0.87       205\n","weighted avg       0.87      0.87      0.87       205\n","\n","\n","‚úÖ Decision Tree completed!\n","\n","######################################################################\n","MODEL 3/6: K-NEAREST NEIGHBORS\n","######################################################################\n","\n","======================================================================\n","üîÑ Training: K-Nearest Neighbors\n","======================================================================\n","‚úÖ Model trained successfully\n","\n","üìä Evaluation Metrics:\n","   Accuracy:  0.8634\n","   AUC:       0.9629\n","   Precision: 0.8738\n","   Recall:    0.8571\n","   F1 Score:  0.8654\n","   MCC:       0.7269\n","\n","üî¢ Confusion Matrix:\n","   TN:  87  |  FP:  13\n","   FN:  15  |  TP:  90\n","\n","üìã Classification Report:\n","              precision    recall  f1-score   support\n","\n","  No Disease       0.85      0.87      0.86       100\n","     Disease       0.87      0.86      0.87       105\n","\n","    accuracy                           0.86       205\n","   macro avg       0.86      0.86      0.86       205\n","weighted avg       0.86      0.86      0.86       205\n","\n","\n","‚úÖ K-Nearest Neighbors completed!\n","\n","######################################################################\n","MODEL 4/6: NAIVE BAYES\n","######################################################################\n","\n","======================================================================\n","üîÑ Training: Naive Bayes\n","======================================================================\n","‚úÖ Model trained successfully\n","\n","üìä Evaluation Metrics:\n","   Accuracy:  0.8293\n","   AUC:       0.9043\n","   Precision: 0.8070\n","   Recall:    0.8762\n","   F1 Score:  0.8402\n","   MCC:       0.6602\n","\n","üî¢ Confusion Matrix:\n","   TN:  78  |  FP:  22\n","   FN:  13  |  TP:  92\n","\n","üìã Classification Report:\n","              precision    recall  f1-score   support\n","\n","  No Disease       0.86      0.78      0.82       100\n","     Disease       0.81      0.88      0.84       105\n","\n","    accuracy                           0.83       205\n","   macro avg       0.83      0.83      0.83       205\n","weighted avg       0.83      0.83      0.83       205\n","\n","\n","‚úÖ Naive Bayes completed!\n","\n","######################################################################\n","MODEL 5/6: RANDOM FOREST\n","######################################################################\n","\n","======================================================================\n","üîÑ Training: Random Forest\n","======================================================================\n","‚úÖ Model trained successfully\n","\n","üìä Evaluation Metrics:\n","   Accuracy:  0.9220\n","   AUC:       0.9708\n","   Precision: 0.9009\n","   Recall:    0.9524\n","   F1 Score:  0.9259\n","   MCC:       0.8450\n","\n","üî¢ Confusion Matrix:\n","   TN:  89  |  FP:  11\n","   FN:   5  |  TP: 100\n","\n","üìã Classification Report:\n","              precision    recall  f1-score   support\n","\n","  No Disease       0.95      0.89      0.92       100\n","     Disease       0.90      0.95      0.93       105\n","\n","    accuracy                           0.92       205\n","   macro avg       0.92      0.92      0.92       205\n","weighted avg       0.92      0.92      0.92       205\n","\n","\n","‚úÖ Random Forest completed!\n","\n","######################################################################\n","MODEL 6/6: XGBOOST\n","######################################################################\n","\n","======================================================================\n","üîÑ Training: XGBoost\n","======================================================================\n","‚úÖ Model trained successfully\n","\n","üìä Evaluation Metrics:\n","   Accuracy:  0.9756\n","   AUC:       0.9868\n","   Precision: 0.9808\n","   Recall:    0.9714\n","   F1 Score:  0.9761\n","   MCC:       0.9512\n","\n","üî¢ Confusion Matrix:\n","   TN:  98  |  FP:   2\n","   FN:   3  |  TP: 102\n","\n","üìã Classification Report:\n","              precision    recall  f1-score   support\n","\n","  No Disease       0.97      0.98      0.98       100\n","     Disease       0.98      0.97      0.98       105\n","\n","    accuracy                           0.98       205\n","   macro avg       0.98      0.98      0.98       205\n","weighted avg       0.98      0.98      0.98       205\n","\n","\n","‚úÖ XGBoost completed!\n","\n","======================================================================\n","‚úÖ ALL 6 MODELS TRAINED SUCCESSFULLY!\n","======================================================================\n","\n","======================================================================\n","üìä MODEL COMPARISON TABLE (ALL 6 MODELS)\n","======================================================================\n","\n","              Model  Accuracy    AUC  Precision  Recall     F1    MCC\n","Logistic Regression    0.8098 0.9298     0.7619  0.9143 0.8312 0.6309\n","      Decision Tree    0.8732 0.9326     0.8624  0.8952 0.8785 0.7465\n","K-Nearest Neighbors    0.8634 0.9629     0.8738  0.8571 0.8654 0.7269\n","        Naive Bayes    0.8293 0.9043     0.8070  0.8762 0.8402 0.6602\n","      Random Forest    0.9220 0.9708     0.9009  0.9524 0.9259 0.8450\n","            XGBoost    0.9756 0.9868     0.9808  0.9714 0.9761 0.9512\n","\n","‚úÖ Comparison table saved: model/model_comparison.csv\n","\n","======================================================================\n","üèÜ BEST PERFORMING MODELS BY METRIC\n","======================================================================\n","Accuracy     : XGBoost                   (0.9756)\n","AUC          : XGBoost                   (0.9868)\n","Precision    : XGBoost                   (0.9808)\n","Recall       : XGBoost                   (0.9714)\n","F1           : XGBoost                   (0.9761)\n","MCC          : XGBoost                   (0.9512)\n","\n","======================================================================\n","ü•á OVERALL BEST MODEL (by average performance)\n","======================================================================\n","Best Model: XGBoost\n","Average Score: 0.9737\n","\n","======================================================================\n","üíæ SAVING REQUIRED FILES FOR STREAMLIT APP\n","======================================================================\n","‚úÖ Saved: model/logistic_regression.pkl\n","‚úÖ Saved: model/decision_tree.pkl\n","‚úÖ Saved: model/k_nearest_neighbors.pkl\n","‚úÖ Saved: model/naive_bayes.pkl\n","‚úÖ Saved: model/random_forest.pkl\n","‚úÖ Saved: model/xgboost.pkl\n","‚úÖ Saved: model/scaler.pkl\n","‚úÖ Saved: model/feature_names.pkl\n","‚úÖ Saved: model/model_comparison.csv\n","‚úÖ Saved: data/test_sample.csv (for demo purposes)\n","\n","‚úÖ All required files saved!\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}